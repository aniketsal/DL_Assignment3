{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing Libraries\n\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport pandas as pd\n\n# Load Training, Validation, and Test Data\n\ndf = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv',names = [\"English\",'Hindi'],header = None)\ndf_val=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_valid.csv',names=[\"English\",\"Hindi\"],header=None)\ndf_test=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_test.csv',names=[\"English\",\"Hindi\"],header=None)\n\nmaxlength_english=0\nmaxlength_hindi=0\n\n# Encoder Dictionary Creation\nhindi_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\nenglish_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\n\n\n# Make dictionary for English alphabets\nenglish_alphabets = 'abcdefghijklmnopqrstuvwxyz'\nfor idx, alphabet in enumerate(english_alphabets):\n    english_to_index[alphabet] = idx + 3\n\n# Make dictionary for Hindi characters\nhindi_characters = set()\nfor x in range(len(df)):\n    english_word=df.iloc[x]['English']\n    hindi_word = df.iloc[x]['Hindi']\n    maxlength_english=max(maxlength_english,len(english_word))\n    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n    hindi_characters.update(hindi_word) \n\n\nfor x in range(len(df_test)):\n    english_word=df_test.iloc[x]['English']\n    hindi_word = df_test.iloc[x]['Hindi']\n    maxlength_english=max(maxlength_english,len(english_word))\n    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n    hindi_characters.update(hindi_word) \n\nstart = 3\nfor i, char in enumerate(hindi_characters):\n    hindi_to_index[char] = start + i\n\n# Printing the created dictionaries\nprint(hindi_to_index)\nprint(english_to_index)\nmaxlength_hindi+=3\n\n# Decoder Dictionary Creation\nindex_to_hindi = {v: k for k, v in hindi_to_index.items()}\nindex_to_english = {v: k for k, v in english_to_index.items()}\nprint(index_to_english)\nprint(index_to_hindi)\n\n\n#functions to create the encodings required for English and hindi words\ndef encode_words_english(language,df):\n    encoded_words=[]\n    maxlength=maxlength_english+1\n    to_index=english_to_index\n    \n    for _, row in df.iterrows():\n        language_word = row['English']\n        word = torch.zeros(maxlength, dtype=torch.long)+2\n        for idx, char in enumerate(language_word):\n            word[idx] = to_index[char]\n        word[len(language_word)]=to_index['EOS_token']\n        encoded_words.append(word)\n    encoded_words = torch.stack(encoded_words)\n    return encoded_words\n\ndef encode_words_hindi(language,df):\n    encoded_words=[]\n    maxlength=maxlength_hindi\n    to_index=hindi_to_index\n    \n    for _, row in df.iterrows():\n        language_word = row['Hindi']\n        word = torch.zeros(maxlength, dtype=torch.long)+2\n        word[0]=to_index['SOS_token']\n        for idx, char in enumerate(language_word):\n            word[idx+1] = to_index[char]\n        word[len(language_word)]=to_index['EOS_token']\n        encoded_words.append(word)\n    encoded_words = torch.stack(encoded_words)\n    return encoded_words\n\n\n#contains encoding for training ,validation and testing data.\nenglish_encoded_words=encode_words_english('English',df)\nhindi_encoded_words=encode_words_hindi('Hindi',df)\nenglish_encoded_words_val=encode_words_english('English',df_val)\nhindi_encoded_words_val=encode_words_hindi('Hindi',df_val)\nenglish_encoded_words_test=encode_words_english('English',df_test)\nhindi_encoded_words_test=encode_words_hindi('Hindi',df_test)\n","metadata":{"_uuid":"69af4d90-8b30-48d1-a865-54a04d770a18","_cell_guid":"be97d6ad-d4a6-4e31-b2c0-d790a2261dde","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T17:23:57.007517Z","iopub.execute_input":"2023-05-21T17:23:57.007894Z","iopub.status.idle":"2023-05-21T17:24:16.752934Z","shell.execute_reply.started":"2023-05-21T17:23:57.007864Z","shell.execute_reply":"2023-05-21T17:24:16.752032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'ऋ': 3, 'ब': 4, 'औ': 5, 'झ': 6, 'ज': 7, 'छ': 8, '्': 9, 'ङ': 10, 'ॉ': 11, 'ं': 12, 'ी': 13, 'फ': 14, 'ऑ': 15, 'त': 16, 'ौ': 17, 'ख': 18, 'थ': 19, 'क': 20, 'ह': 21, 'इ': 22, 'आ': 23, 'ु': 24, 'द': 25, 'ि': 26, 'य': 27, 'ँ': 28, 'ऽ': 29, 'भ': 30, 'ः': 31, 'ई': 32, 'ट': 33, 'ॅ': 34, 'ळ': 35, 'ऐ': 36, 'ऊ': 37, 'ओ': 38, '़': 39, 'ञ': 40, 'म': 41, 'ू': 42, 'ल': 43, 'न': 44, 'अ': 45, 'ठ': 46, 'ढ': 47, 'ध': 48, 'च': 49, 'उ': 50, 'ए': 51, 'प': 52, 'ण': 53, 'र': 54, 'ग': 55, 'ो': 56, 'व': 57, 'ष': 58, 'स': 59, 'ॊ': 60, 'ा': 61, 'ड': 62, 'ृ': 63, 'े': 64, 'श': 65, 'ै': 66, 'घ': 67}\n{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'ऋ', 4: 'ब', 5: 'औ', 6: 'झ', 7: 'ज', 8: 'छ', 9: '्', 10: 'ङ', 11: 'ॉ', 12: 'ं', 13: 'ी', 14: 'फ', 15: 'ऑ', 16: 'त', 17: 'ौ', 18: 'ख', 19: 'थ', 20: 'क', 21: 'ह', 22: 'इ', 23: 'आ', 24: 'ु', 25: 'द', 26: 'ि', 27: 'य', 28: 'ँ', 29: 'ऽ', 30: 'भ', 31: 'ः', 32: 'ई', 33: 'ट', 34: 'ॅ', 35: 'ळ', 36: 'ऐ', 37: 'ऊ', 38: 'ओ', 39: '़', 40: 'ञ', 41: 'म', 42: 'ू', 43: 'ल', 44: 'न', 45: 'अ', 46: 'ठ', 47: 'ढ', 48: 'ध', 49: 'च', 50: 'उ', 51: 'ए', 52: 'प', 53: 'ण', 54: 'र', 55: 'ग', 56: 'ो', 57: 'व', 58: 'ष', 59: 'स', 60: 'ॊ', 61: 'ा', 62: 'ड', 63: 'ृ', 64: 'े', 65: 'श', 66: 'ै', 67: 'घ'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#function to reshape the hidden layer \ndef reshape_arr(x,num_layers):\n    for i in range(1,num_layers,+2):\n        if(i==1):tmp=x[i]\n        else:tmp+=x[i]\n    tmp1=tmp.repeat(num_layers,1,1)\n    return tmp1","metadata":{"_uuid":"f6ceec79-bc5d-4e58-94a2-e1a77b437ede","_cell_guid":"4b9d8ea6-7d02-41cc-acaf-763e7e0f2c8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T17:24:16.754706Z","iopub.execute_input":"2023-05-21T17:24:16.755042Z","iopub.status.idle":"2023-05-21T17:24:16.760731Z","shell.execute_reply.started":"2023-05-21T17:24:16.755012Z","shell.execute_reply":"2023-05-21T17:24:16.759726Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#importing wandb\nimport wandb\nwandb.login(key='a9d4ee5e3628e01c0f6f0fa50e59e7be9438d147')\nwandb.init(project=\"dl_assignment3\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:24:16.762214Z","iopub.execute_input":"2023-05-21T17:24:16.762771Z","iopub.status.idle":"2023-05-21T17:24:51.353978Z","shell.execute_reply.started":"2023-05-21T17:24:16.762740Z","shell.execute_reply":"2023-05-21T17:24:51.353141Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_172420-y3qpb4wf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf' target=\"_blank\">stellar-snowball-143</a></strong> to <a href='https://wandb.ai/cs22m013/dl_assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs22m013/dl_assignment3' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf</a>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7bc035b237f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Q3.Implemented RNN,GRU and LSTM**","metadata":{}},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size,embedding_size,num_layers,drop,cell_type,bidirection=True):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding_size=embedding_size\n        self. num_layers=num_layers\n        self.dropout=nn.Dropout(drop)\n        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n        self.bidirectional=bidirection\n        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.cell_type=cell_type\n    def forward(self, input):\n        #input:(seq_length,N)\n        input=input.T\n#         print(\"einput \",input.shape)\n        embedded = self.dropout(self.embedding(input))\n#         print(\"eembed \",embedded.shape)\n        #embedded:(seq_length,N,embedding_size)\n        if(self.cell_type==\"LSTM\"):\n            output,(hidden,cell)=self.lstm(embedded)\n#             print('encodero',output.shape)\n#             print('enchid',hidden.shape)\n#             print('enccell',cell.shape)\n            if(self.bidirectional):\n                hidden=reshape_arr(hidden,self.num_layers)\n                cell=reshape_arr(cell,self.num_layers)\n            return output,(hidden,cell)\n            \n        if(self.cell_type==\"GRU\"):\n            output, hidden = self.gru(embedded)\n\n        if(self.cell_type==\"RNN\"):\n            output,hidden=self.rnn(embedded)\n             \n        if(self.bidirectional):\n            hidden=reshape_arr(hidden,self.num_layers)\n        return  output,hidden\n        \n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, input_size,hidden_size, output_size,embedding_size,num_layers,drop,cell_type):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding_size=embedding_size\n        self. num_layers=num_layers\n        self.dropout=nn.Dropout(drop)\n        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.cell_type=cell_type\n        \n        self.fc_out = nn.Linear(hidden_size, output_size).to(device)\n\n    def forward(self, input,hidden,cell):\n        \n        input=input.T\n        \n        embedded = self.dropout(self.embedding(input))\n        #embedded = [1, batch size,embedding_size]\n        \n        if(self.cell_type==\"RNN\"):\n            output,hidden = self.rnn(embedded,hidden)\n        if(self.cell_type=='GRU'):\n            output,hidden = self.gru(embedded,hidden)\n        if(self.cell_type==\"LSTM\"):\n            output,(hidden,cell)=self.lstm(embedded,(hidden,cell))\n            prediction = self.fc_out(output)\n            return prediction,hidden,cell\n        #output:[1,batch_size,hidden_size]\n        prediction = self.fc_out(output)\n        return prediction, hidden\n\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=maxlength_english):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder,cell_type):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.softmax = nn.Softmax(dim=2)\n        self.cell_type=cell_type\n\n\n        \n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n#         print(source.shape)\n#         print(target.shape)\n        target_vocab_size = len(hindi_to_index)\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)        \n        if(self.cell_type=='LSTM'):\n            output, (hidden,cell) = self.encoder.forward(source)\n        else:\n            output, hidden = self.encoder.forward(source)\n\n#         print(\"output\",\"hidden\")\n#         print(output,hidden)\n        x = target[:,0].reshape(batch_size,1)\n        #print(target_len)\n        for t in range(1, target_len):\n            if(self.cell_type=='LSTM'):\n                output, hidden,cell = self.decoder.forward(x, hidden,cell)\n            else:\n                output, hidden = self.decoder.forward(x, hidden,None)\n            \n#             print(\"dout\",output.shape)\n            outputs[t] = output.squeeze(0)\n            teacher_force = random.random() < teacher_forcing_ratio\n            output = self.softmax(output)\n#             print(\"doutput \",output.shape)\n            top1 = torch.argmax(output,dim = 2)\n#             print(\"top1 \",top1.shape)\n            x = target[:,t].reshape(batch_size,1) if teacher_force else top1.T\n        return outputs","metadata":{"_uuid":"7f7fe7b2-9f29-4435-b8f3-5df2970fc323","_cell_guid":"9ba5f987-4b34-4201-a8ab-9d3b6addb00c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T17:33:17.182336Z","iopub.status.idle":"2023-05-21T17:33:17.183142Z","shell.execute_reply.started":"2023-05-21T17:33:17.182896Z","shell.execute_reply":"2023-05-21T17:33:17.182921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file1 = open(\"predictions_vanilla.txt\",\"a\")\n#file for predictions of the test dataset contains hindi predicted and english predicted.","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:33:17.184424Z","iopub.status.idle":"2023-05-21T17:33:17.185235Z","shell.execute_reply.started":"2023-05-21T17:33:17.184992Z","shell.execute_reply":"2023-05-21T17:33:17.185015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Q4.b)Provide sample inputs from the test data and predictions made by your best model (more marks for presenting this grid creatively). Implemented using write_to_file function**","metadata":{}},{"cell_type":"code","source":"#to convert the tensors to calculate accuracy\ndef calculate_predictions(output,target):\n    output1=nn.Softmax(dim=2)(output[1:])\n    predictions=torch.argmax(output1,dim=2)\n    pred=predictions.T\n    target1=target[:,1:]\n    return pred,target1\n\n#for printing the prediction and target in text file.\ndef write_to_file(pred,target):\n    pred_s=''\n    for i in pred:\n        if(i in index_to_hindi):\n            pred_s+=index_to_hindi[i]\n    pred_target=''\n    for i in target:\n        if(i in index_to_hindi):\n            pred_target+=index_to_hindi[i]\n    file1.write(pred_s+\"        \"+pred_target)\n\n#to calculate accuracy \ndef calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,teacher_forcing_ratio):\n    correct=0\n    total_loss=0\n    loss_function=nn.CrossEntropyLoss(reduction='sum')\n    \n    for i in range(0,len(english_encoded_words),batch_size):\n        src=english_encoded_words[i:i+batch_size].to(device)\n        target=hindi_encoded_words[i:i+batch_size].to(device)\n        output=model.forward(src,target,0)\n        pred,target1=calculate_predictions(output,target)\n        out = output[1:].reshape(-1, output.shape[2])\n        target2 = target[:,1:].T.reshape(-1)\n        loss = loss_function(out, target2)\n        total_loss += loss.item()\n        for t in range(len(pred)):\n            if(False):\n                write_to_file(pred[t],target1[t])\n            if(torch.equal(pred[t],target1[t])):\n                correct+=1\n    return correct,total_loss\n\n\n\n\n\ndef train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type):\n    input_size_encoder=len(english_to_index)\n    input_size_decoder=len(hindi_to_index)\n    output_size=len(hindi_to_index)\n    encoder_net=EncoderRNN(input_size_encoder, hidden_size,embedding_size,num_layers,enc_dropout,cell_type).to(device)\n    decoder_net=DecoderRNN(input_size_decoder,hidden_size,output_size,embedding_size,num_layers,dec_dropout,cell_type).to(device)\n    model=Seq2Seq(encoder_net,decoder_net,cell_type).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    correct_predictions=0\n    correct_predictions_val=0\n    loss_function=nn.CrossEntropyLoss(reduction='sum')\n    for epoch in range(num_epochs):\n        print(epoch)\n        total_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n        for i in range(0,len(english_encoded_words),batch_size):\n            src=english_encoded_words[i:i+batch_size].to(device)\n            target=hindi_encoded_words[i:i+batch_size].to(device)\n            \n            output=model(src,target)\n            output1=nn.Softmax(dim=2)(output[1:])\n\n            predictions=torch.argmax(output1,dim=2)\n   \n            out = output[1:].reshape(-1, output.shape[2])\n            target1 = target[:,1:].T.reshape(-1)\n   \n            optimizer.zero_grad()\n            loss = loss_function(out, target1)\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            optimizer.step()\n        #to find the loss and accuracy\n        correct_predictions,training_loss=calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,0)\n        correct_predictions_val,val_loss=calculate_accuracy(model,english_encoded_words_val,hindi_encoded_words_val,batch_size,0)\n        correct_predictions_test,test_loss=calculate_accuracy(model,english_encoded_words_test,hindi_encoded_words_test,batch_size,0)\n        \n        Training_loss=total_loss/(len(english_encoded_words)*maxlength_hindi)\n        Validation_loss=val_loss/(len(english_encoded_words_val)*maxlength_hindi)\n        Validation_accuracy=(correct_predictions_val/len(english_encoded_words_val)*100)\n        Test_accuracy=(correct_predictions_test/len(english_encoded_words_test)*100)\n        Training_accuracy=(correct_predictions/51200)*100\n        print(\"Training_accuracy:\",Training_accuracy)\n        print(\"Validation_accuracy:\",Validation_accuracy)\n        print(\"Test_accuracy:\",Test_accuracy)\n        wandb.log({'Training_accuracy':Training_accuracy,'Epoch':epoch+1,'Training_loss':Training_loss,'Validation_loss':Validation_loss,'Validation_accuracy':Validation_accuracy})\n","metadata":{"_uuid":"ba27bfa3-e0ec-447a-90b9-e55f43558ec1","_cell_guid":"d9b89705-0eef-4c16-b77e-5c607f6da4ce","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T17:33:17.186806Z","iopub.status.idle":"2023-05-21T17:33:17.187435Z","shell.execute_reply.started":"2023-05-21T17:33:17.187177Z","shell.execute_reply":"2023-05-21T17:33:17.187200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Q4)Best configuration for without attetion**","metadata":{}},{"cell_type":"code","source":"#best config in attention\nnum_layers=4\nenc_dropout=0.3\ndec_dropout=0.3\nnum_epochs=1\nlearning_rate=0.001\nbatch_size=512\nhidden_size=1024\nembedding_size=256\ncell_type=\"LSTM\"\ntrain(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def withattention():\n    wandb.init(project='dl_assignment3')\n    config = wandb.config\n    wandb.run.name = \"cell_type_{}bidirec{}layers{}batchsize{}hidden{}embedding{}learning_rate{}\".format(config.cell_type,config.bidirectional,config.no_of_layers,config.batchsize,config.hidden_size,config.input_embedding_size,config.learning_rate)\n    hidden_size = config.hidden_size\n    embedding_size = config.input_embedding_size\n    num_layers = config.no_of_layers\n    num_epochs = config.epochs\n    batch_size = config.batchsize\n    enc_dropout = config.dropout\n    dec_dropout=config.dropout\n    cell_type=config.cell_type\n    learning_rate=config.learning_rate\n    train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:33:17.188852Z","iopub.status.idle":"2023-05-21T17:33:17.189654Z","shell.execute_reply.started":"2023-05-21T17:33:17.189406Z","shell.execute_reply":"2023-05-21T17:33:17.189428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q3. Sweep Configurations ","metadata":{}},{"cell_type":"code","source":"sweep_configuration = {\n    'method' : 'bayes',\n    'metric' : { 'goal' : 'maximize',\n    'name' : 'Validation_accuracy'},\n    'parameters':{\n        'learning_rate': {'values':[0.001,0.002]},\n        'batchsize' : {'values' : [128,256,512,1024]},\n        'input_embedding_size' : {'values' : [256,512,1024]},\n        'no_of_layers' : {'values' : [2,3,4]},\n        'hidden_size' : {'values' : [256,512,1024]},\n        'cell_type' : {'values' : ['RNN','LSTM','GRU']},\n        'bidirectional' : {'values' : ['Yes']},\n        'dropout' : {'values' : [0.2,0.3,0.4]},\n        'epochs' : {'values' : [10,15,18]}\n    }\n}\nsweep_id = wandb.sweep(sweep = sweep_configuration,project = 'dl_assignment3')\nwandb.agent(sweep_id,function=withattention,count = 1)","metadata":{"_uuid":"6cda99a3-fd59-4bea-8319-18c8c2e91606","_cell_guid":"2b6b8adf-d50a-44c3-87d0-c61433294f6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T17:33:17.191204Z","iopub.status.idle":"2023-05-21T17:33:17.191685Z","shell.execute_reply.started":"2023-05-21T17:33:17.191441Z","shell.execute_reply":"2023-05-21T17:33:17.191463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}