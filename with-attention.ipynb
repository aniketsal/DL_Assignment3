{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"be97d6ad-d4a6-4e31-b2c0-d790a2261dde","_uuid":"69af4d90-8b30-48d1-a865-54a04d770a18","collapsed":false,"execution":{"iopub.execute_input":"2023-05-21T17:23:57.007894Z","iopub.status.busy":"2023-05-21T17:23:57.007517Z","iopub.status.idle":"2023-05-21T17:24:16.752934Z","shell.execute_reply":"2023-05-21T17:24:16.752032Z","shell.execute_reply.started":"2023-05-21T17:23:57.007864Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'ऋ': 3, 'ब': 4, 'औ': 5, 'झ': 6, 'ज': 7, 'छ': 8, '्': 9, 'ङ': 10, 'ॉ': 11, 'ं': 12, 'ी': 13, 'फ': 14, 'ऑ': 15, 'त': 16, 'ौ': 17, 'ख': 18, 'थ': 19, 'क': 20, 'ह': 21, 'इ': 22, 'आ': 23, 'ु': 24, 'द': 25, 'ि': 26, 'य': 27, 'ँ': 28, 'ऽ': 29, 'भ': 30, 'ः': 31, 'ई': 32, 'ट': 33, 'ॅ': 34, 'ळ': 35, 'ऐ': 36, 'ऊ': 37, 'ओ': 38, '़': 39, 'ञ': 40, 'म': 41, 'ू': 42, 'ल': 43, 'न': 44, 'अ': 45, 'ठ': 46, 'ढ': 47, 'ध': 48, 'च': 49, 'उ': 50, 'ए': 51, 'प': 52, 'ण': 53, 'र': 54, 'ग': 55, 'ो': 56, 'व': 57, 'ष': 58, 'स': 59, 'ॊ': 60, 'ा': 61, 'ड': 62, 'ृ': 63, 'े': 64, 'श': 65, 'ै': 66, 'घ': 67}\n","{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n","{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n","{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'ऋ', 4: 'ब', 5: 'औ', 6: 'झ', 7: 'ज', 8: 'छ', 9: '्', 10: 'ङ', 11: 'ॉ', 12: 'ं', 13: 'ी', 14: 'फ', 15: 'ऑ', 16: 'त', 17: 'ौ', 18: 'ख', 19: 'थ', 20: 'क', 21: 'ह', 22: 'इ', 23: 'आ', 24: 'ु', 25: 'द', 26: 'ि', 27: 'य', 28: 'ँ', 29: 'ऽ', 30: 'भ', 31: 'ः', 32: 'ई', 33: 'ट', 34: 'ॅ', 35: 'ळ', 36: 'ऐ', 37: 'ऊ', 38: 'ओ', 39: '़', 40: 'ञ', 41: 'म', 42: 'ू', 43: 'ल', 44: 'न', 45: 'अ', 46: 'ठ', 47: 'ढ', 48: 'ध', 49: 'च', 50: 'उ', 51: 'ए', 52: 'प', 53: 'ण', 54: 'र', 55: 'ग', 56: 'ो', 57: 'व', 58: 'ष', 59: 'स', 60: 'ॊ', 61: 'ा', 62: 'ड', 63: 'ृ', 64: 'े', 65: 'श', 66: 'ै', 67: 'घ'}\n"]}],"source":["# Importing Libraries\n","\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","import pandas as pd\n","\n","# Load Training, Validation, and Test Data\n","\n","df = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv',names = [\"English\",'Hindi'],header = None)\n","df_val=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_valid.csv',names=[\"English\",\"Hindi\"],header=None)\n","df_test=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_test.csv',names=[\"English\",\"Hindi\"],header=None)\n","\n","maxlength_english=0\n","maxlength_hindi=0\n","\n","# Encoder Dictionary Creation\n","hindi_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\n","english_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\n","\n","\n","# Make dictionary for English alphabets\n","english_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n","for idx, alphabet in enumerate(english_alphabets):\n","    english_to_index[alphabet] = idx + 3\n","\n","# Make dictionary for Hindi characters\n","hindi_characters = set()\n","for x in range(len(df)):\n","    english_word=df.iloc[x]['English']\n","    hindi_word = df.iloc[x]['Hindi']\n","    maxlength_english=max(maxlength_english,len(english_word))\n","    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n","    hindi_characters.update(hindi_word) \n","\n","\n","for x in range(len(df_test)):\n","    english_word=df_test.iloc[x]['English']\n","    hindi_word = df_test.iloc[x]['Hindi']\n","    maxlength_english=max(maxlength_english,len(english_word))\n","    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n","    hindi_characters.update(hindi_word) \n","\n","start = 3\n","for i, char in enumerate(hindi_characters):\n","    hindi_to_index[char] = start + i\n","\n","# Printing the created dictionaries\n","print(hindi_to_index)\n","print(english_to_index)\n","maxlength_hindi+=3\n","\n","# Decoder Dictionary Creation\n","index_to_hindi = {v: k for k, v in hindi_to_index.items()}\n","index_to_english = {v: k for k, v in english_to_index.items()}\n","print(index_to_english)\n","print(index_to_hindi)\n","\n","\n","#functions to create the encodings required for English and hindi words\n","def encode_words_english(language,df):\n","    encoded_words=[]\n","    maxlength=maxlength_english+1\n","    to_index=english_to_index\n","    \n","    for _, row in df.iterrows():\n","        language_word = row['English']\n","        word = torch.zeros(maxlength, dtype=torch.long)+2\n","        for idx, char in enumerate(language_word):\n","            word[idx] = to_index[char]\n","        word[len(language_word)]=to_index['EOS_token']\n","        encoded_words.append(word)\n","    encoded_words = torch.stack(encoded_words)\n","    return encoded_words\n","\n","def encode_words_hindi(language,df):\n","    encoded_words=[]\n","    maxlength=maxlength_hindi\n","    to_index=hindi_to_index\n","    \n","    for _, row in df.iterrows():\n","        language_word = row['Hindi']\n","        word = torch.zeros(maxlength, dtype=torch.long)+2\n","        word[0]=to_index['SOS_token']\n","        for idx, char in enumerate(language_word):\n","            word[idx+1] = to_index[char]\n","        word[len(language_word)]=to_index['EOS_token']\n","        encoded_words.append(word)\n","    encoded_words = torch.stack(encoded_words)\n","    return encoded_words\n","\n","\n","#contains encoding for training ,validation and testing data.\n","english_encoded_words=encode_words_english('English',df)\n","hindi_encoded_words=encode_words_hindi('Hindi',df)\n","english_encoded_words_val=encode_words_english('English',df_val)\n","hindi_encoded_words_val=encode_words_hindi('Hindi',df_val)\n","english_encoded_words_test=encode_words_english('English',df_test)\n","hindi_encoded_words_test=encode_words_hindi('Hindi',df_test)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"4b9d8ea6-7d02-41cc-acaf-763e7e0f2c8a","_uuid":"f6ceec79-bc5d-4e58-94a2-e1a77b437ede","collapsed":false,"execution":{"iopub.execute_input":"2023-05-21T17:24:16.755042Z","iopub.status.busy":"2023-05-21T17:24:16.754706Z","iopub.status.idle":"2023-05-21T17:24:16.760731Z","shell.execute_reply":"2023-05-21T17:24:16.759726Z","shell.execute_reply.started":"2023-05-21T17:24:16.755012Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#function to reshape the hidden layer \n","def reshape_arr(x,num_layers):\n","    for i in range(1,num_layers,+2):\n","        if(i==1):tmp=x[i]\n","        else:tmp+=x[i]\n","    tmp1=tmp.repeat(num_layers,1,1)\n","    return tmp1"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T17:24:16.762771Z","iopub.status.busy":"2023-05-21T17:24:16.762214Z","iopub.status.idle":"2023-05-21T17:24:51.353978Z","shell.execute_reply":"2023-05-21T17:24:51.353141Z","shell.execute_reply.started":"2023-05-21T17:24:16.762740Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m013\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.15.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_172420-y3qpb4wf</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf' target=\"_blank\">stellar-snowball-143</a></strong> to <a href='https://wandb.ai/cs22m013/dl_assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22m013/dl_assignment3' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf' target=\"_blank\">https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m013/dl_assignment3/runs/y3qpb4wf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7bc035b237f0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#importing wandb\n","import wandb\n","wandb.login(key='a9d4ee5e3628e01c0f6f0fa50e59e7be9438d147')\n","wandb.init(project=\"dl_assignment3\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Q3.Implemented RNN,GRU and LSTM**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ba5f987-4b34-4201-a8ab-9d3b6addb00c","_uuid":"7f7fe7b2-9f29-4435-b8f3-5df2970fc323","collapsed":false,"execution":{"iopub.status.busy":"2023-05-21T17:33:17.182336Z","iopub.status.idle":"2023-05-21T17:33:17.183142Z","shell.execute_reply":"2023-05-21T17:33:17.182921Z","shell.execute_reply.started":"2023-05-21T17:33:17.182896Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size,embedding_size,num_layers,drop,cell_type,bidirection=True):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding_size=embedding_size\n","        self. num_layers=num_layers\n","        self.dropout=nn.Dropout(drop)\n","        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n","        self.bidirectional=bidirection\n","        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n","        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n","        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n","        self.cell_type=cell_type\n","    def forward(self, input):\n","        #input:(seq_length,N)\n","        input=input.T\n","#         print(\"einput \",input.shape)\n","        embedded = self.dropout(self.embedding(input))\n","#         print(\"eembed \",embedded.shape)\n","        #embedded:(seq_length,N,embedding_size)\n","        if(self.cell_type==\"LSTM\"):\n","            output,(hidden,cell)=self.lstm(embedded)\n","#             print('encodero',output.shape)\n","#             print('enchid',hidden.shape)\n","#             print('enccell',cell.shape)\n","            if(self.bidirectional):\n","                hidden=reshape_arr(hidden,self.num_layers)\n","                cell=reshape_arr(cell,self.num_layers)\n","            return output,(hidden,cell)\n","            \n","        if(self.cell_type==\"GRU\"):\n","            output, hidden = self.gru(embedded)\n","\n","        if(self.cell_type==\"RNN\"):\n","            output,hidden=self.rnn(embedded)\n","             \n","        if(self.bidirectional):\n","            hidden=reshape_arr(hidden,self.num_layers)\n","        return  output,hidden\n","        \n","\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, input_size,hidden_size, output_size,embedding_size,num_layers,drop,cell_type):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding_size=embedding_size\n","        self. num_layers=num_layers\n","        self.dropout=nn.Dropout(drop)\n","        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n","        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n","        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n","        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n","        self.cell_type=cell_type\n","        \n","        self.fc_out = nn.Linear(hidden_size, output_size).to(device)\n","\n","    def forward(self, input,hidden,cell):\n","        \n","        input=input.T\n","        \n","        embedded = self.dropout(self.embedding(input))\n","        #embedded = [1, batch size,embedding_size]\n","        \n","        if(self.cell_type==\"RNN\"):\n","            output,hidden = self.rnn(embedded,hidden)\n","        if(self.cell_type=='GRU'):\n","            output,hidden = self.gru(embedded,hidden)\n","        if(self.cell_type==\"LSTM\"):\n","            output,(hidden,cell)=self.lstm(embedded,(hidden,cell))\n","            prediction = self.fc_out(output)\n","            return prediction,hidden,cell\n","        #output:[1,batch_size,hidden_size]\n","        prediction = self.fc_out(output)\n","        return prediction, hidden\n","\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=maxlength_english):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,cell_type):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.softmax = nn.Softmax(dim=2)\n","        self.cell_type=cell_type\n","\n","\n","        \n","    def forward(self, source, target, teacher_forcing_ratio=0.5):\n","        batch_size = source.shape[0]\n","        target_len = target.shape[1]\n","#         print(source.shape)\n","#         print(target.shape)\n","        target_vocab_size = len(hindi_to_index)\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)        \n","        if(self.cell_type=='LSTM'):\n","            output, (hidden,cell) = self.encoder.forward(source)\n","        else:\n","            output, hidden = self.encoder.forward(source)\n","\n","#         print(\"output\",\"hidden\")\n","#         print(output,hidden)\n","        x = target[:,0].reshape(batch_size,1)\n","        #print(target_len)\n","        for t in range(1, target_len):\n","            if(self.cell_type=='LSTM'):\n","                output, hidden,cell = self.decoder.forward(x, hidden,cell)\n","            else:\n","                output, hidden = self.decoder.forward(x, hidden,None)\n","            \n","#             print(\"dout\",output.shape)\n","            outputs[t] = output.squeeze(0)\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            output = self.softmax(output)\n","#             print(\"doutput \",output.shape)\n","            top1 = torch.argmax(output,dim = 2)\n","#             print(\"top1 \",top1.shape)\n","            x = target[:,t].reshape(batch_size,1) if teacher_force else top1.T\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:33:17.184424Z","iopub.status.idle":"2023-05-21T17:33:17.185235Z","shell.execute_reply":"2023-05-21T17:33:17.185015Z","shell.execute_reply.started":"2023-05-21T17:33:17.184992Z"},"trusted":true},"outputs":[],"source":["file1 = open(\"predictions_attention.txt\",\"a\")\n","#file for predictions of the test dataset contains hindi predicted and english predicted."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" **Q4.b)Provide sample inputs from the test data and predictions made by your best model (more marks for presenting this grid creatively). Implemented using write_to_file function**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9b89705-0eef-4c16-b77e-5c607f6da4ce","_uuid":"ba27bfa3-e0ec-447a-90b9-e55f43558ec1","collapsed":false,"execution":{"iopub.status.busy":"2023-05-21T17:33:17.186806Z","iopub.status.idle":"2023-05-21T17:33:17.187435Z","shell.execute_reply":"2023-05-21T17:33:17.187200Z","shell.execute_reply.started":"2023-05-21T17:33:17.187177Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["#to convert the tensors to calculate accuracy\n","def calculate_predictions(output,target):\n","    output1=nn.Softmax(dim=2)(output[1:])\n","    predictions=torch.argmax(output1,dim=2)\n","    pred=predictions.T\n","    target1=target[:,1:]\n","    return pred,target1\n","\n","#for printing the prediction and target in text file.\n","def write_to_file(pred,target):\n","    pred_s=''\n","    for i in pred:\n","        if(i in index_to_hindi):\n","            pred_s+=index_to_hindi[i]\n","    pred_target=''\n","    for i in target:\n","        if(i in index_to_hindi):\n","            pred_target+=index_to_hindi[i]\n","    file1.write(pred_s+\"        \"+pred_target)\n","\n","#to calculate accuracy \n","def calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,teacher_forcing_ratio):\n","    correct=0\n","    total_loss=0\n","    loss_function=nn.CrossEntropyLoss(reduction='sum')\n","    \n","    for i in range(0,len(english_encoded_words),batch_size):\n","        src=english_encoded_words[i:i+batch_size].to(device)\n","        target=hindi_encoded_words[i:i+batch_size].to(device)\n","        output=model.forward(src,target,0)\n","        pred,target1=calculate_predictions(output,target)\n","        out = output[1:].reshape(-1, output.shape[2])\n","        target2 = target[:,1:].T.reshape(-1)\n","        loss = loss_function(out, target2)\n","        total_loss += loss.item()\n","        for t in range(len(pred)):\n","            if(False):\n","                write_to_file(pred[t],target1[t])\n","            if(torch.equal(pred[t],target1[t])):\n","                correct+=1\n","    return correct,total_loss\n","\n","\n","\n","\n","\n","def train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type):\n","    input_size_encoder=len(english_to_index)\n","    input_size_decoder=len(hindi_to_index)\n","    output_size=len(hindi_to_index)\n","    encoder_net=EncoderRNN(input_size_encoder, hidden_size,embedding_size,num_layers,enc_dropout,cell_type).to(device)\n","    decoder_net=DecoderRNN(input_size_decoder,hidden_size,output_size,embedding_size,num_layers,dec_dropout,cell_type).to(device)\n","    model=Seq2Seq(encoder_net,decoder_net,cell_type).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    correct_predictions=0\n","    correct_predictions_val=0\n","    loss_function=nn.CrossEntropyLoss(reduction='sum')\n","    for epoch in range(num_epochs):\n","        print(epoch)\n","        total_loss = 0\n","        correct_predictions = 0\n","        total_predictions = 0\n","        for i in range(0,len(english_encoded_words),batch_size):\n","            src=english_encoded_words[i:i+batch_size].to(device)\n","            target=hindi_encoded_words[i:i+batch_size].to(device)\n","            \n","            output=model(src,target)\n","            output1=nn.Softmax(dim=2)(output[1:])\n","\n","            predictions=torch.argmax(output1,dim=2)\n","   \n","            out = output[1:].reshape(-1, output.shape[2])\n","            target1 = target[:,1:].T.reshape(-1)\n","   \n","            optimizer.zero_grad()\n","            loss = loss_function(out, target1)\n","            total_loss += loss.item()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","            optimizer.step()\n","        #to find the loss and accuracy\n","        correct_predictions,training_loss=calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,0)\n","        correct_predictions_val,val_loss=calculate_accuracy(model,english_encoded_words_val,hindi_encoded_words_val,batch_size,0)\n","        correct_predictions_test,test_loss=calculate_accuracy(model,english_encoded_words_test,hindi_encoded_words_test,batch_size,0)\n","        \n","        Training_loss=total_loss/(len(english_encoded_words)*maxlength_hindi)\n","        Validation_loss=val_loss/(len(english_encoded_words_val)*maxlength_hindi)\n","        Validation_accuracy=(correct_predictions_val/len(english_encoded_words_val)*100)\n","        Test_accuracy=(correct_predictions_test/len(english_encoded_words_test)*100)\n","        Training_accuracy=(correct_predictions/51200)*100\n","        print(\"Training_accuracy:\",Training_accuracy)\n","        print(\"Validation_accuracy:\",Validation_accuracy)\n","        print(\"Test_accuracy:\",Test_accuracy)\n","        wandb.log({'Training_accuracy':Training_accuracy,'Epoch':epoch+1,'Training_loss':Training_loss,'Validation_loss':Validation_loss,'Validation_accuracy':Validation_accuracy})\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Q4)Best configuration for without attetion**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#best config in attention\n","num_layers=4\n","enc_dropout=0.3\n","dec_dropout=0.3\n","num_epochs=1\n","learning_rate=0.001\n","batch_size=512\n","hidden_size=1024\n","embedding_size=256\n","cell_type=\"LSTM\"\n","train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:33:17.188852Z","iopub.status.idle":"2023-05-21T17:33:17.189654Z","shell.execute_reply":"2023-05-21T17:33:17.189428Z","shell.execute_reply.started":"2023-05-21T17:33:17.189406Z"},"trusted":true},"outputs":[],"source":["def withattention():\n","    wandb.init(project='dl_assignment3')\n","    config = wandb.config\n","    wandb.run.name = \"cell_type_{}bidirec{}layers{}batchsize{}hidden{}embedding{}learning_rate{}\".format(config.cell_type,config.bidirectional,config.no_of_layers,config.batchsize,config.hidden_size,config.input_embedding_size,config.learning_rate)\n","    hidden_size = config.hidden_size\n","    embedding_size = config.input_embedding_size\n","    num_layers = config.no_of_layers\n","    num_epochs = config.epochs\n","    batch_size = config.batchsize\n","    enc_dropout = config.dropout\n","    dec_dropout=config.dropout\n","    cell_type=config.cell_type\n","    learning_rate=config.learning_rate\n","    train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)\n","    \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Q3. Sweep Configurations "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b6b8adf-d50a-44c3-87d0-c61433294f6a","_uuid":"6cda99a3-fd59-4bea-8319-18c8c2e91606","collapsed":false,"execution":{"iopub.status.busy":"2023-05-21T17:33:17.191204Z","iopub.status.idle":"2023-05-21T17:33:17.191685Z","shell.execute_reply":"2023-05-21T17:33:17.191463Z","shell.execute_reply.started":"2023-05-21T17:33:17.191441Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["sweep_configuration = {\n","    'method' : 'bayes',\n","    'metric' : { 'goal' : 'maximize',\n","    'name' : 'Validation_accuracy'},\n","    'parameters':{\n","        'learning_rate': {'values':[0.001,0.002]},\n","        'batchsize' : {'values' : [128,256,512,1024]},\n","        'input_embedding_size' : {'values' : [256,512,1024]},\n","        'no_of_layers' : {'values' : [2,3,4]},\n","        'hidden_size' : {'values' : [256,512,1024]},\n","        'cell_type' : {'values' : ['RNN','LSTM','GRU']},\n","        'bidirectional' : {'values' : ['Yes']},\n","        'dropout' : {'values' : [0.2,0.3,0.4]},\n","        'epochs' : {'values' : [10,15,18]}\n","    }\n","}\n","sweep_id = wandb.sweep(sweep = sweep_configuration,project = 'dl_assignment3')\n","wandb.agent(sweep_id,function=withattention,count = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
