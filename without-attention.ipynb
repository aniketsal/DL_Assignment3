{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing Libraries\n\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport pandas as pd\n\n# Load Training, Validation, and Test Data\n\ndf = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv',names = [\"English\",'Hindi'],header = None)\ndf_val=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_valid.csv',names=[\"English\",\"Hindi\"],header=None)\ndf_test=pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_test.csv',names=[\"English\",\"Hindi\"],header=None)\n\nmaxlength_english=0\nmaxlength_hindi=0\n\n# Encoder Dictionary Creation\nhindi_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\nenglish_to_index = {'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2}\n\n\n# Make dictionary for English alphabets\nenglish_alphabets = 'abcdefghijklmnopqrstuvwxyz'\nfor idx, alphabet in enumerate(english_alphabets):\n    english_to_index[alphabet] = idx + 3\n\n# Make dictionary for Hindi characters\nhindi_characters = set()\nfor x in range(len(df)):\n    english_word=df.iloc[x]['English']\n    hindi_word = df.iloc[x]['Hindi']\n    maxlength_english=max(maxlength_english,len(english_word))\n    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n    hindi_characters.update(hindi_word) \n\n\nfor x in range(len(df_test)):\n    english_word=df_test.iloc[x]['English']\n    hindi_word = df_test.iloc[x]['Hindi']\n    maxlength_english=max(maxlength_english,len(english_word))\n    maxlength_hindi=max(maxlength_hindi,len(hindi_word))\n    hindi_characters.update(hindi_word) \n\nstart = 3\nfor i, char in enumerate(hindi_characters):\n    hindi_to_index[char] = start + i\n\n# Printing the created dictionaries\nprint(hindi_to_index)\nprint(english_to_index)\nmaxlength_hindi+=3\n\n# Decoder Dictionary Creation\nindex_to_hindi = {v: k for k, v in hindi_to_index.items()}\nindex_to_english = {v: k for k, v in english_to_index.items()}\nprint(index_to_english)\nprint(index_to_hindi)\n\n\n#functions to create the encodings required for English and hindi words\ndef encode_words_english(language,df):\n    encoded_words=[]\n    maxlength=maxlength_english+1\n    to_index=english_to_index\n    \n    for _, row in df.iterrows():\n        language_word = row['English']\n        word = torch.zeros(maxlength, dtype=torch.long)+2\n        for idx, char in enumerate(language_word):\n            word[idx] = to_index[char]\n        word[len(language_word)]=to_index['EOS_token']\n        encoded_words.append(word)\n    encoded_words = torch.stack(encoded_words)\n    return encoded_words\n\ndef encode_words_hindi(language,df):\n    encoded_words=[]\n    maxlength=maxlength_hindi\n    to_index=hindi_to_index\n    \n    for _, row in df.iterrows():\n        language_word = row['Hindi']\n        word = torch.zeros(maxlength, dtype=torch.long)+2\n        word[0]=to_index['SOS_token']\n        for idx, char in enumerate(language_word):\n            word[idx+1] = to_index[char]\n        word[len(language_word)]=to_index['EOS_token']\n        encoded_words.append(word)\n    encoded_words = torch.stack(encoded_words)\n    return encoded_words\n\n\n#contains encoding for training ,validation and testing data.\nenglish_encoded_words=encode_words_english('English',df)\nhindi_encoded_words=encode_words_hindi('Hindi',df)\nenglish_encoded_words_val=encode_words_english('English',df_val)\nhindi_encoded_words_val=encode_words_hindi('Hindi',df_val)\nenglish_encoded_words_test=encode_words_english('English',df_test)\nhindi_encoded_words_test=encode_words_hindi('Hindi',df_test)\n","metadata":{"_uuid":"69af4d90-8b30-48d1-a865-54a04d770a18","_cell_guid":"be97d6ad-d4a6-4e31-b2c0-d790a2261dde","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T13:38:09.065916Z","iopub.execute_input":"2023-05-21T13:38:09.069068Z","iopub.status.idle":"2023-05-21T13:38:31.681267Z","shell.execute_reply.started":"2023-05-21T13:38:09.069013Z","shell.execute_reply":"2023-05-21T13:38:31.680188Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'आ': 3, 'ञ': 4, 'त': 5, 'े': 6, 'ऽ': 7, 'ॊ': 8, 'ऐ': 9, 'न': 10, 'ई': 11, 'ं': 12, '़': 13, 'ए': 14, 'घ': 15, '्': 16, 'उ': 17, 'स': 18, 'क': 19, 'ी': 20, 'म': 21, 'ज': 22, 'छ': 23, 'ओ': 24, 'ट': 25, 'ब': 26, 'य': 27, 'ख': 28, 'र': 29, 'द': 30, 'ष': 31, 'ि': 32, 'ः': 33, 'फ': 34, 'च': 35, 'ऑ': 36, 'ॅ': 37, 'ँ': 38, 'ळ': 39, 'ऊ': 40, 'ु': 41, 'ो': 42, 'ग': 43, 'श': 44, 'ल': 45, 'ध': 46, 'ा': 47, 'ण': 48, 'अ': 49, 'ौ': 50, 'ू': 51, 'ॉ': 52, 'ढ': 53, 'ठ': 54, 'ृ': 55, 'ऋ': 56, 'भ': 57, 'ह': 58, 'औ': 59, 'ङ': 60, 'व': 61, 'प': 62, 'ड': 63, 'इ': 64, 'ै': 65, 'थ': 66, 'झ': 67}\n{'SOS_token': 0, 'EOS_token': 1, 'PAD_token': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n{0: 'SOS_token', 1: 'EOS_token', 2: 'PAD_token', 3: 'आ', 4: 'ञ', 5: 'त', 6: 'े', 7: 'ऽ', 8: 'ॊ', 9: 'ऐ', 10: 'न', 11: 'ई', 12: 'ं', 13: '़', 14: 'ए', 15: 'घ', 16: '्', 17: 'उ', 18: 'स', 19: 'क', 20: 'ी', 21: 'म', 22: 'ज', 23: 'छ', 24: 'ओ', 25: 'ट', 26: 'ब', 27: 'य', 28: 'ख', 29: 'र', 30: 'द', 31: 'ष', 32: 'ि', 33: 'ः', 34: 'फ', 35: 'च', 36: 'ऑ', 37: 'ॅ', 38: 'ँ', 39: 'ळ', 40: 'ऊ', 41: 'ु', 42: 'ो', 43: 'ग', 44: 'श', 45: 'ल', 46: 'ध', 47: 'ा', 48: 'ण', 49: 'अ', 50: 'ौ', 51: 'ू', 52: 'ॉ', 53: 'ढ', 54: 'ठ', 55: 'ृ', 56: 'ऋ', 57: 'भ', 58: 'ह', 59: 'औ', 60: 'ङ', 61: 'व', 62: 'प', 63: 'ड', 64: 'इ', 65: 'ै', 66: 'थ', 67: 'झ'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#function to reshape the hidden layer \ndef reshape_arr(x,num_layers):\n    for i in range(1,num_layers,+2):\n        if(i==1):tmp=x[i]\n        else:tmp+=x[i]\n    tmp1=tmp.repeat(num_layers,1,1)\n    return tmp1","metadata":{"_uuid":"f6ceec79-bc5d-4e58-94a2-e1a77b437ede","_cell_guid":"4b9d8ea6-7d02-41cc-acaf-763e7e0f2c8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T13:38:31.682963Z","iopub.execute_input":"2023-05-21T13:38:31.683306Z","iopub.status.idle":"2023-05-21T13:38:31.688428Z","shell.execute_reply.started":"2023-05-21T13:38:31.683280Z","shell.execute_reply":"2023-05-21T13:38:31.687543Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#importing wandb\nimport wandb\nwandb.login(key='a9d4ee5e3628e01c0f6f0fa50e59e7be9438d147')\nwandb.init(project=\"dl_assignment3\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T13:38:31.689739Z","iopub.execute_input":"2023-05-21T13:38:31.690382Z","iopub.status.idle":"2023-05-21T13:38:31.706012Z","shell.execute_reply.started":"2023-05-21T13:38:31.690335Z","shell.execute_reply":"2023-05-21T13:38:31.704953Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'\\nimport wandb\\nwandb.login(key=\\'a9d4ee5e3628e01c0f6f0fa50e59e7be9438d147\\')\\nwandb.init(project=\"dl_assignment3\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size,embedding_size,num_layers,drop,cell_type,bidirection=True):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding_size=embedding_size\n        self. num_layers=num_layers\n        self.dropout=nn.Dropout(drop)\n        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n        self.bidirectional=bidirection\n        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False,bidirectional=bidirection).to(device)\n        self.cell_type=cell_type\n    def forward(self, input):\n        #input:(seq_length,N)\n        input=input.T\n#         print(\"einput \",input.shape)\n        embedded = self.dropout(self.embedding(input))\n#         print(\"eembed \",embedded.shape)\n        #embedded:(seq_length,N,embedding_size)\n        if(self.cell_type==\"LSTM\"):\n            output,(hidden,cell)=self.lstm(embedded)\n#             print('encodero',output.shape)\n#             print('enchid',hidden.shape)\n#             print('enccell',cell.shape)\n            if(self.bidirectional):\n                hidden=reshape_arr(hidden,self.num_layers)\n                cell=reshape_arr(cell,self.num_layers)\n            return output,(hidden,cell)\n            \n        if(self.cell_type==\"GRU\"):\n            output, hidden = self.gru(embedded)\n\n        if(self.cell_type==\"RNN\"):\n            output,hidden=self.rnn(embedded)\n             \n        if(self.bidirectional):\n            hidden=reshape_arr(hidden,self.num_layers)\n        return  output,hidden\n        \n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, input_size,hidden_size, output_size,embedding_size,num_layers,drop,cell_type):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding_size=embedding_size\n        self. num_layers=num_layers\n        self.dropout=nn.Dropout(drop)\n        self.embedding = nn.Embedding(input_size, embedding_size).to(device)\n        self.lstm=nn.LSTM(embedding_size,hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.rnn = nn.RNN(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=drop,batch_first=False).to(device)\n        self.cell_type=cell_type\n        \n        self.fc_out = nn.Linear(hidden_size, output_size).to(device)\n\n    def forward(self, input,hidden,cell):\n        \n        input=input.T\n        \n        embedded = self.dropout(self.embedding(input))\n        #embedded = [1, batch size,embedding_size]\n        \n        if(self.cell_type==\"RNN\"):\n            output,hidden = self.rnn(embedded,hidden)\n        if(self.cell_type=='GRU'):\n            output,hidden = self.gru(embedded,hidden)\n        if(self.cell_type==\"LSTM\"):\n            output,(hidden,cell)=self.lstm(embedded,(hidden,cell))\n            prediction = self.fc_out(output)\n            return prediction,hidden,cell\n        #output:[1,batch_size,hidden_size]\n        prediction = self.fc_out(output)\n        return prediction, hidden\n\n\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder,cell_type):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.softmax = nn.Softmax(dim=2)\n        self.cell_type=cell_type\n\n\n        \n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n#         print(source.shape)\n#         print(target.shape)\n        target_vocab_size = len(hindi_to_index)\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)        \n        if(self.cell_type=='LSTM'):\n            output, (hidden,cell) = self.encoder.forward(source)\n        else:\n            output, hidden = self.encoder.forward(source)\n\n#         print(\"output\",\"hidden\")\n#         print(output,hidden)\n        x = target[:,0].reshape(batch_size,1)\n        #print(target_len)\n        for t in range(1, target_len):\n            if(self.cell_type=='LSTM'):\n                output, hidden,cell = self.decoder.forward(x, hidden,cell)\n            else:\n                output, hidden = self.decoder.forward(x, hidden,None)\n            \n#             print(\"dout\",output.shape)\n            outputs[t] = output.squeeze(0)\n            teacher_force = random.random() < teacher_forcing_ratio\n            output = self.softmax(output)\n#             print(\"doutput \",output.shape)\n            top1 = torch.argmax(output,dim = 2)\n#             print(\"top1 \",top1.shape)\n            x = target[:,t].reshape(batch_size,1) if teacher_force else top1.T\n        return outputs","metadata":{"_uuid":"7f7fe7b2-9f29-4435-b8f3-5df2970fc323","_cell_guid":"9ba5f987-4b34-4201-a8ab-9d3b6addb00c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T13:38:31.708762Z","iopub.execute_input":"2023-05-21T13:38:31.709081Z","iopub.status.idle":"2023-05-21T13:38:31.735259Z","shell.execute_reply.started":"2023-05-21T13:38:31.709051Z","shell.execute_reply":"2023-05-21T13:38:31.734224Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"file1 = open(\"predictions_vanilla.txt\",\"a\")\n#file for predictions of the test dataset contains hindi predicted and english predicted.","metadata":{"execution":{"iopub.status.busy":"2023-05-21T13:38:31.736783Z","iopub.execute_input":"2023-05-21T13:38:31.737282Z","iopub.status.idle":"2023-05-21T13:38:31.749062Z","shell.execute_reply.started":"2023-05-21T13:38:31.737250Z","shell.execute_reply":"2023-05-21T13:38:31.748185Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#to convert the tensors to calculate accuracy\ndef calculate_predictions(output,target):\n    output1=nn.Softmax(dim=2)(output[1:])\n    predictions=torch.argmax(output1,dim=2)\n    pred=predictions.T\n    target1=target[:,1:]\n    return pred,target1\n\n#for printing the prediction and target in text file.\ndef write_to_file(pred,target):\n    pred_s=''\n    for i in pred:\n        if(i in index_to_hindi):\n            pred_s+=index_to_hindi[i]\n    pred_target=''\n    for i in target:\n        if(i in index_to_hindi):\n            pred_target+=index_to_hindi[i]\n    file1.write(pred_s+\"        \"+pred_target)\n\n#to calculate accuracy \ndef calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,teacher_forcing_ratio):\n    correct=0\n    total_loss=0\n    loss_function=nn.CrossEntropyLoss(reduction='sum')\n    \n    for i in range(0,len(english_encoded_words),batch_size):\n        src=english_encoded_words[i:i+batch_size].to(device)\n        target=hindi_encoded_words[i:i+batch_size].to(device)\n        output=model.forward(src,target,0)\n        pred,target1=calculate_predictions(output,target)\n        out = output[1:].reshape(-1, output.shape[2])\n        target2 = target[:,1:].T.reshape(-1)\n        loss = loss_function(out, target2)\n        total_loss += loss.item()\n        for t in range(len(pred)):\n            if(False):\n                write_to_file(pred[t],target1[t])\n            if(torch.equal(pred[t],target1[t])):\n                correct+=1\n    return correct,total_loss\n\n\n\n\n\ndef train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type):\n    input_size_encoder=len(english_to_index)\n    input_size_decoder=len(hindi_to_index)\n    output_size=len(hindi_to_index)\n    encoder_net=EncoderRNN(input_size_encoder, hidden_size,embedding_size,num_layers,enc_dropout,cell_type).to(device)\n    decoder_net=DecoderRNN(input_size_decoder,hidden_size,output_size,embedding_size,num_layers,dec_dropout,cell_type).to(device)\n    model=Seq2Seq(encoder_net,decoder_net,cell_type).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    correct_predictions=0\n    correct_predictions_val=0\n    loss_function=nn.CrossEntropyLoss(reduction='sum')\n    for epoch in range(num_epochs):\n        print(epoch)\n        total_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n        for i in range(0,len(english_encoded_words),batch_size):\n            src=english_encoded_words[i:i+batch_size].to(device)\n            target=hindi_encoded_words[i:i+batch_size].to(device)\n            \n            output=model(src,target)\n            output1=nn.Softmax(dim=2)(output[1:])\n\n            predictions=torch.argmax(output1,dim=2)\n   \n            out = output[1:].reshape(-1, output.shape[2])\n            target1 = target[:,1:].T.reshape(-1)\n   \n            optimizer.zero_grad()\n            loss = loss_function(out, target1)\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            optimizer.step()\n        #to find the loss and accuracy\n        correct_predictions,training_loss=calculate_accuracy(model,english_encoded_words,hindi_encoded_words,batch_size,0)\n        correct_predictions_val,val_loss=calculate_accuracy(model,english_encoded_words_val,hindi_encoded_words_val,batch_size,0)\n        correct_predictions_test,test_loss=calculate_accuracy(model,english_encoded_words_test,hindi_encoded_words_test,batch_size,0)\n        \n        Training_loss=total_loss/(len(english_encoded_words)*maxlength_hindi)\n        Validation_loss=val_loss/(len(english_encoded_words_val)*maxlength_hindi)\n        Validation_accuracy=(correct_predictions_val/len(english_encoded_words_val)*100)\n        Test_accuracy=(correct_predictions_test/len(english_encoded_words_test)*100)\n        Training_accuracy=(correct_predictions/51200)*100\n        print(\"Training_accuracy:\",Training_accuracy)\n        print(\"Validation_accuracy:\",Validation_accuracy)\n        print(\"Test_accuracy:\",Test_accuracy)\n        wandb.log({'Training_accuracy':Training_accuracy,'Epoch':epoch+1,'Training_loss':Training_loss,'Validation_loss':Validation_loss,'Validation_accuracy':Validation_accuracy})\n","metadata":{"_uuid":"ba27bfa3-e0ec-447a-90b9-e55f43558ec1","_cell_guid":"d9b89705-0eef-4c16-b77e-5c607f6da4ce","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T13:39:15.474809Z","iopub.execute_input":"2023-05-21T13:39:15.475182Z","iopub.status.idle":"2023-05-21T13:39:15.495572Z","shell.execute_reply.started":"2023-05-21T13:39:15.475131Z","shell.execute_reply":"2023-05-21T13:39:15.494620Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#best config in vanilla.\nnum_layers=4\nenc_dropout=0.3\ndec_dropout=0.3\nnum_epochs=10\nlearning_rate=0.001\nbatch_size=512\nhidden_size=1024\nembedding_size=256\ncell_type=\"LSTM\"\ntrain(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T13:39:19.806909Z","iopub.execute_input":"2023-05-21T13:39:19.807293Z","iopub.status.idle":"2023-05-21T14:02:18.761233Z","shell.execute_reply.started":"2023-05-21T13:39:19.807261Z","shell.execute_reply":"2023-05-21T14:02:18.760186Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0\nTest_accuracy: 13.4033203125\n1\nTest_accuracy: 27.9052734375\n2\nTest_accuracy: 33.0810546875\n3\nTest_accuracy: 37.9638671875\n4\nTest_accuracy: 37.9638671875\n5\nTest_accuracy: 38.818359375\n6\nTest_accuracy: 39.9658203125\n7\nTest_accuracy: 38.6962890625\n8\nTest_accuracy: 38.57421875\n9\nTest_accuracy: 39.013671875\n","output_type":"stream"}]},{"cell_type":"code","source":"def withoutattention():\n    wandb.init(project='dl_assignment3')\n    config = wandb.config\n    wandb.run.name = \"cell_type_{}bidirec{}layers{}batchsize{}hidden{}embedding{}learning_rate{}\".format(config.cell_type,config.bidirectional,config.no_of_layers,config.batchsize,config.hidden_size,config.input_embedding_size,config.learning_rate)\n    hidden_size = config.hidden_size\n    embedding_size = config.input_embedding_size\n    num_layers = config.no_of_layers\n    num_epochs = config.epochs\n    batch_size = config.batchsize\n    enc_dropout = config.dropout\n    dec_dropout=config.dropout\n    cell_type=config.cell_type\n    learning_rate=config.learning_rate\n    train(num_layers,enc_dropout,dec_dropout,num_epochs,learning_rate,batch_size,embedding_size,hidden_size,cell_type)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T13:38:31.775691Z","iopub.status.idle":"2023-05-21T13:38:31.776751Z","shell.execute_reply.started":"2023-05-21T13:38:31.776511Z","shell.execute_reply":"2023-05-21T13:38:31.776536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sweep_configuration = {\n    'method' : 'bayes',\n    'metric' : { 'goal' : 'maximize',\n    'name' : 'Validation_accuracy'},\n    'parameters':{\n        'learning_rate': {'values':[0.001,0.002]},\n        'batchsize' : {'values' : [128,256,512,1024]},\n        'input_embedding_size' : {'values' : [256,512,1024]},\n        'no_of_layers' : {'values' : [2,3,4]},\n        'hidden_size' : {'values' : [256,512,1024]},\n        'cell_type' : {'values' : ['RNN','LSTM','GRU']},\n        'bidirectional' : {'values' : ['Yes']},\n        'dropout' : {'values' : [0.2,0.3,0.4]},\n        'epochs' : {'values' : [10,15,18]}\n    }\n}\nsweep_id = wandb.sweep(sweep = sweep_configuration,project = 'dl_assignment3')\nwandb.agent(sweep_id,function=withoutattention,count = 1)","metadata":{"_uuid":"6cda99a3-fd59-4bea-8319-18c8c2e91606","_cell_guid":"2b6b8adf-d50a-44c3-87d0-c61433294f6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-21T13:38:31.778197Z","iopub.status.idle":"2023-05-21T13:38:31.779163Z","shell.execute_reply.started":"2023-05-21T13:38:31.778871Z","shell.execute_reply":"2023-05-21T13:38:31.778895Z"},"trusted":true},"execution_count":null,"outputs":[]}]}